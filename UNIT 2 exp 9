# DQN with Prioritized Experience Replay (NumPy Only)

import numpy as np
import random

# -----------------------------
# 1. Environment Definition
# -----------------------------
states = 5      # arm positions
actions = 3     # move left, move right, pick/place

def step(state, action):
    reward = -1
    next_state = state

    if action == 0:  # move left
        next_state = max(0, state - 1)
    elif action == 1:  # move right
        next_state = min(states - 1, state + 1)
    elif action == 2 and state == states - 1:  # pick & place success
        reward = 10

    return next_state, reward

# -----------------------------
# 2. Q-Network (Linear Model)
# -----------------------------
Q = np.zeros((states, actions))

# -----------------------------
# 3. Prioritized Replay Buffer
# -----------------------------
memory = []
priorities = []
capacity = 1000

def store(experience, error):
    if len(memory) >= capacity:
        memory.pop(0)
        priorities.pop(0)
    memory.append(experience)
    priorities.append(abs(error) + 1e-5)

def sample(batch_size):
    probs = priorities / np.sum(priorities)
    indices = np.random.choice(len(memory), batch_size, p=probs)
    return [memory[i] for i in indices]

# -----------------------------
# 4. Training Loop
# -----------------------------
episodes = 300
alpha = 0.1
gamma = 0.95
epsilon = 1.0

for ep in range(episodes):
    state = random.randint(0, states - 1)

    for _ in range(10):
        if random.random() < epsilon:
            action = random.randint(0, actions - 1)
        else:
            action = np.argmax(Q[state])

        next_state, reward = step(state, action)

        td_error = reward + gamma * np.max(Q[next_state]) - Q[state, action]
        store((state, action, reward, next_state), td_error)

        if len(memory) > 10:
            batch = sample(5)
            for s, a, r, ns in batch:
                Q[s, a] += alpha * (r + gamma * np.max(Q[ns]) - Q[s, a])

        state = next_state

    epsilon = max(0.1, epsilon * 0.99)

# -----------------------------
# 5. Final Output
# -----------------------------
print("Training completed.\n")
print("Learned Q-Table:")
print(Q)
